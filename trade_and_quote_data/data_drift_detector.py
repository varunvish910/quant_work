"""
Data Drift Detection System

This module detects when new data differs significantly from training data,
helping identify when models need retraining due to distribution changes.

Author: Generated by Claude Code
Date: 2025-10-01
"""

import numpy as np
import pandas as pd
from scipy import stats
from typing import Dict, List, Tuple, Optional
import logging
from dataclasses import dataclass
import warnings

logger = logging.getLogger(__name__)

@dataclass
class DriftResult:
    """Result of drift detection for a single feature"""
    feature_name: str
    drift_detected: bool
    drift_score: float
    test_statistic: float
    p_value: float
    test_method: str

class DataDriftDetector:
    """
    Detects data drift using statistical tests to compare
    training and new data distributions.
    """
    
    def __init__(self, 
                 ks_threshold: float = 0.05,
                 chi_square_threshold: float = 0.05,
                 feature_drift_threshold: float = 0.1):
        """
        Initialize drift detector with configurable thresholds.
        
        Args:
            ks_threshold: P-value threshold for Kolmogorov-Smirnov test
            chi_square_threshold: P-value threshold for Chi-square test
            feature_drift_threshold: Fraction of features that must drift to trigger alert
        """
        self.ks_threshold = ks_threshold
        self.chi_square_threshold = chi_square_threshold
        self.feature_drift_threshold = feature_drift_threshold
        
        # Store reference distributions
        self.reference_data = None
        self.reference_stats = {}
        
    def fit(self, reference_data: np.ndarray, feature_names: Optional[List[str]] = None):
        """
        Fit the drift detector on reference (training) data.
        
        Args:
            reference_data: Training data to use as reference
            feature_names: Names of features (optional)
        """
        self.reference_data = reference_data.copy()
        
        if feature_names is None:
            feature_names = [f"feature_{i}" for i in range(reference_data.shape[1])]
        
        self.feature_names = feature_names
        
        # Calculate reference statistics
        self.reference_stats = {}
        for i, feature_name in enumerate(feature_names):
            feature_data = reference_data[:, i]
            self.reference_stats[feature_name] = {
                'mean': np.mean(feature_data),
                'std': np.std(feature_data),
                'min': np.min(feature_data),
                'max': np.max(feature_data),
                'percentiles': np.percentile(feature_data, [25, 50, 75])
            }
        
        logger.info(f"Fitted drift detector on {len(feature_names)} features")
        
    def detect_drift(self, new_data: np.ndarray) -> Tuple[bool, List[DriftResult]]:
        """
        Detect drift in new data compared to reference data.
        
        Args:
            new_data: New data to compare against reference
            
        Returns:
            Tuple of (overall_drift_detected, list_of_drift_results)
        """
        if self.reference_data is None:
            raise ValueError("Must call fit() before detect_drift()")
            
        if new_data.shape[1] != self.reference_data.shape[1]:
            raise ValueError("New data must have same number of features as reference data")
        
        drift_results = []
        
        for i, feature_name in enumerate(self.feature_names):
            ref_feature = self.reference_data[:, i]
            new_feature = new_data[:, i]
            
            # Detect drift for this feature
            drift_result = self._detect_feature_drift(
                ref_feature, new_feature, feature_name
            )
            drift_results.append(drift_result)
        
        # Overall drift detection
        drifted_features = sum(1 for result in drift_results if result.drift_detected)
        drift_fraction = drifted_features / len(drift_results)
        overall_drift = drift_fraction > self.feature_drift_threshold
        
        if overall_drift:
            logger.warning(f"Data drift detected! {drifted_features}/{len(drift_results)} features drifted "
                         f"({drift_fraction:.2%} > {self.feature_drift_threshold:.2%})")
        else:
            logger.info(f"No significant drift detected ({drift_fraction:.2%} features drifted)")
        
        return overall_drift, drift_results
    
    def _detect_feature_drift(self, ref_data: np.ndarray, new_data: np.ndarray, 
                             feature_name: str) -> DriftResult:
        """
        Detect drift for a single feature using appropriate statistical test.
        
        Args:
            ref_data: Reference feature data
            new_data: New feature data
            feature_name: Name of the feature
            
        Returns:
            DriftResult object with test results
        """
        # Choose appropriate test based on data characteristics
        if self._is_continuous(ref_data) and self._is_continuous(new_data):
            return self._ks_test(ref_data, new_data, feature_name)
        else:
            return self._chi_square_test(ref_data, new_data, feature_name)
    
    def _is_continuous(self, data: np.ndarray, threshold: int = 20) -> bool:
        """Check if data appears to be continuous (vs categorical)"""
        unique_values = len(np.unique(data))
        return unique_values > threshold
    
    def _ks_test(self, ref_data: np.ndarray, new_data: np.ndarray, 
                 feature_name: str) -> DriftResult:
        """
        Perform Kolmogorov-Smirnov test for continuous features.
        """
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                ks_stat, p_value = stats.ks_2samp(ref_data, new_data)
            
            drift_detected = p_value < self.ks_threshold
            drift_score = 1 - p_value  # Higher score = more drift
            
            return DriftResult(
                feature_name=feature_name,
                drift_detected=drift_detected,
                drift_score=drift_score,
                test_statistic=ks_stat,
                p_value=p_value,
                test_method="kolmogorov_smirnov"
            )
        except Exception as e:
            logger.warning(f"KS test failed for {feature_name}: {e}")
            return DriftResult(
                feature_name=feature_name,
                drift_detected=False,
                drift_score=0.0,
                test_statistic=0.0,
                p_value=1.0,
                test_method="kolmogorov_smirnov_failed"
            )
    
    def _chi_square_test(self, ref_data: np.ndarray, new_data: np.ndarray, 
                        feature_name: str) -> DriftResult:
        """
        Perform Chi-square test for categorical features.
        """
        try:
            # Create contingency table
            ref_counts = pd.Series(ref_data).value_counts()
            new_counts = pd.Series(new_data).value_counts()
            
            # Align indices
            all_values = set(ref_counts.index) | set(new_counts.index)
            ref_aligned = ref_counts.reindex(all_values, fill_value=0)
            new_aligned = new_counts.reindex(all_values, fill_value=0)
            
            # Perform chi-square test
            contingency_table = np.array([ref_aligned.values, new_aligned.values])
            chi2_stat, p_value, _, _ = stats.chi2_contingency(contingency_table)
            
            drift_detected = p_value < self.chi_square_threshold
            drift_score = 1 - p_value
            
            return DriftResult(
                feature_name=feature_name,
                drift_detected=drift_detected,
                drift_score=drift_score,
                test_statistic=chi2_stat,
                p_value=p_value,
                test_method="chi_square"
            )
        except Exception as e:
            logger.warning(f"Chi-square test failed for {feature_name}: {e}")
            return DriftResult(
                feature_name=feature_name,
                drift_detected=False,
                drift_score=0.0,
                test_statistic=0.0,
                p_value=1.0,
                test_method="chi_square_failed"
            )
    
    def get_drift_summary(self, drift_results: List[DriftResult]) -> Dict:
        """
        Generate a summary of drift detection results.
        
        Args:
            drift_results: List of DriftResult objects
            
        Returns:
            Dictionary with drift summary statistics
        """
        total_features = len(drift_results)
        drifted_features = sum(1 for r in drift_results if r.drift_detected)
        
        drift_scores = [r.drift_score for r in drift_results]
        
        summary = {
            'total_features': total_features,
            'drifted_features': drifted_features,
            'drift_percentage': (drifted_features / total_features) * 100,
            'avg_drift_score': np.mean(drift_scores),
            'max_drift_score': np.max(drift_scores),
            'min_drift_score': np.min(drift_scores),
            'features_by_drift': {}
        }
        
        # Group features by drift status
        for result in drift_results:
            status = 'drifted' if result.drift_detected else 'stable'
            if status not in summary['features_by_drift']:
                summary['features_by_drift'][status] = []
            
            summary['features_by_drift'][status].append({
                'name': result.feature_name,
                'score': result.drift_score,
                'p_value': result.p_value,
                'method': result.test_method
            })
        
        return summary
    
    def should_retrain(self, drift_results: List[DriftResult]) -> Tuple[bool, str]:
        """
        Determine if models should be retrained based on drift results.
        
        Args:
            drift_results: List of DriftResult objects
            
        Returns:
            Tuple of (should_retrain, reason)
        """
        drifted_features = sum(1 for r in drift_results if r.drift_detected)
        drift_fraction = drifted_features / len(drift_results)
        
        if drift_fraction > self.feature_drift_threshold:
            reason = (f"Significant drift detected: {drifted_features}/{len(drift_results)} "
                     f"features drifted ({drift_fraction:.2%} > {self.feature_drift_threshold:.2%})")
            return True, reason
        else:
            reason = f"Drift within acceptable limits ({drift_fraction:.2%})"
            return False, reason


def create_drift_detector_with_defaults() -> DataDriftDetector:
    """
    Create a DataDriftDetector with default settings suitable for financial data.
    """
    return DataDriftDetector(
        ks_threshold=0.05,  # 5% significance level
        chi_square_threshold=0.05,  # 5% significance level  
        feature_drift_threshold=0.1  # 10% of features must drift to trigger alert
    )


if __name__ == "__main__":
    # Example usage
    logging.basicConfig(level=logging.INFO)
    
    # Generate sample data
    np.random.seed(42)
    
    # Reference data (normal distribution)
    ref_data = np.random.normal(0, 1, (1000, 5))
    
    # New data with slight drift
    new_data = np.random.normal(0.5, 1.2, (200, 5))  # Mean shift and variance change
    
    # Test drift detection
    detector = create_drift_detector_with_defaults()
    detector.fit(ref_data, feature_names=[f"feature_{i}" for i in range(5)])
    
    overall_drift, results = detector.detect_drift(new_data)
    summary = detector.get_drift_summary(results)
    
    print(f"Overall drift detected: {overall_drift}")
    print(f"Drift summary: {summary}")
    
    should_retrain, reason = detector.should_retrain(results)
    print(f"Should retrain: {should_retrain} - {reason}")