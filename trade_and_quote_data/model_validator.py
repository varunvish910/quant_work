"""
Model Validation Framework for Anomaly Detection

Comprehensive model validation and performance monitoring system
with time series aware validation, performance metrics, and threshold optimization.

Author: Generated by Claude Code
Date: 2025-10-01
"""

import numpy as np
import pandas as pd
from pathlib import Path
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report,
    precision_recall_curve, roc_curve
)
from sklearn.model_selection import TimeSeriesSplit
import matplotlib.pyplot as plt
import seaborn as sns
import json
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)

class ModelValidator:
    """
    Comprehensive validation framework for anomaly detection models
    """
    
    def __init__(self, output_dir: str = "analysis/outputs/validation"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.validation_results = {}
        self.performance_history = []
        
    def time_series_cross_validation(self, X: np.ndarray, y: np.ndarray, 
                                   model, n_splits: int = 5) -> Dict[str, float]:
        """
        Perform time series aware cross-validation
        
        Args:
            X: Feature matrix
            y: Target vector
            model: Model to validate
            n_splits: Number of CV splits
            
        Returns:
            Dictionary with CV performance metrics
        """
        logger.info(f"Running time series CV with {n_splits} splits")
        
        tscv = TimeSeriesSplit(n_splits=n_splits)
        
        cv_scores = {
            'accuracy': [],
            'precision': [],
            'recall': [],
            'f1': [],
            'auc': []
        }
        
        fold_results = []
        
        for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):
            logger.info(f"Processing fold {fold + 1}/{n_splits}")
            
            X_train_fold, X_test_fold = X[train_idx], X[test_idx]
            y_train_fold, y_test_fold = y[train_idx], y[test_idx]
            
            # Clone and fit model
            try:
                from sklearn.base import clone
                fold_model = clone(model)
                fold_model.fit(X_train_fold, y_train_fold)
                
                # Make predictions
                if hasattr(fold_model, 'predict_proba'):
                    y_pred_proba = fold_model.predict_proba(X_test_fold)
                    if y_pred_proba.shape[1] == 2:
                        y_pred_proba = y_pred_proba[:, 1]
                    else:
                        y_pred_proba = y_pred_proba.ravel()
                else:
                    y_pred_proba = fold_model.decision_function(X_test_fold)
                    # Normalize to [0,1] range
                    y_pred_proba = (y_pred_proba - y_pred_proba.min()) / (y_pred_proba.max() - y_pred_proba.min())
                
                y_pred = fold_model.predict(X_test_fold)
                
                # Calculate metrics
                fold_metrics = self._calculate_metrics(y_test_fold, y_pred, y_pred_proba)
                
                for metric, value in fold_metrics.items():
                    if metric in cv_scores:
                        cv_scores[metric].append(value)
                
                fold_results.append({
                    'fold': fold + 1,
                    'train_size': len(X_train_fold),
                    'test_size': len(X_test_fold),
                    'metrics': fold_metrics
                })
                
            except Exception as e:
                logger.warning(f"Fold {fold + 1} failed: {e}")
                continue
        
        # Calculate mean and std for each metric
        cv_summary = {}
        for metric, scores in cv_scores.items():
            if scores:  # Only if we have scores
                cv_summary[f'{metric}_mean'] = np.mean(scores)
                cv_summary[f'{metric}_std'] = np.std(scores)
            else:
                cv_summary[f'{metric}_mean'] = 0.0
                cv_summary[f'{metric}_std'] = 0.0
        
        cv_summary['fold_results'] = fold_results
        
        logger.info(f"CV completed. F1 mean: {cv_summary.get('f1_mean', 0):.3f} Â± {cv_summary.get('f1_std', 0):.3f}")
        
        return cv_summary
    
    def _calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray, 
                          y_pred_proba: Optional[np.ndarray] = None) -> Dict[str, float]:
        """Calculate comprehensive performance metrics"""
        
        metrics = {}
        
        # Convert predictions to 0/1 format (handle -1/1 from anomaly detectors)
        y_pred = np.where(y_pred == -1, 1, y_pred)  # Convert -1 to 1 (anomaly)
        y_pred = np.where(y_pred != 1, 0, 1)  # Ensure only 0/1 values
        
        # Basic metrics
        metrics['accuracy'] = accuracy_score(y_true, y_pred)
        
        # Handle case where there are no positive samples
        if np.sum(y_true) > 0:
            metrics['precision'] = precision_score(y_true, y_pred, zero_division=0)
            metrics['recall'] = recall_score(y_true, y_pred, zero_division=0)
            metrics['f1'] = f1_score(y_true, y_pred, zero_division=0)
        else:
            metrics['precision'] = 0.0
            metrics['recall'] = 0.0
            metrics['f1'] = 0.0
        
        # AUC if probabilities available
        if y_pred_proba is not None and np.sum(y_true) > 0 and len(np.unique(y_true)) > 1:
            try:
                metrics['auc'] = roc_auc_score(y_true, y_pred_proba)
            except ValueError:
                metrics['auc'] = 0.5
        else:
            metrics['auc'] = 0.5
        
        # Confusion matrix components
        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()
        
        metrics['true_positives'] = int(tp)
        metrics['false_positives'] = int(fp) 
        metrics['true_negatives'] = int(tn)
        metrics['false_negatives'] = int(fn)
        
        # Additional metrics
        metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0.0
        metrics['false_positive_rate'] = fp / (fp + tn) if (fp + tn) > 0 else 0.0
        metrics['false_negative_rate'] = fn / (fn + tp) if (fn + tp) > 0 else 0.0
        
        return metrics
    
    def validate_model(self, model, X: np.ndarray, y: np.ndarray, 
                      model_name: str, cv_splits: int = 5) -> Dict[str, Any]:
        """
        Comprehensive validation of a single model
        
        Args:
            model: Trained model to validate
            X: Feature matrix
            y: Target vector
            model_name: Name of the model
            cv_splits: Number of CV splits
            
        Returns:
            Complete validation results
        """
        logger.info(f"Validating model: {model_name}")
        
        validation_result = {
            'model_name': model_name,
            'timestamp': datetime.now().isoformat(),
            'data_info': {
                'n_samples': len(X),
                'n_features': X.shape[1],
                'n_positive': int(np.sum(y)),
                'n_negative': int(len(y) - np.sum(y)),
                'class_balance': float(np.sum(y)) / len(y)
            }
        }
        
        # Time series cross-validation
        cv_results = self.time_series_cross_validation(X, y, model, cv_splits)
        validation_result['cross_validation'] = cv_results
        
        # Full dataset evaluation
        try:
            # Make predictions on full dataset
            y_pred = model.predict(X)
            
            if hasattr(model, 'predict_proba'):
                y_pred_proba = model.predict_proba(X)
                if y_pred_proba.shape[1] == 2:
                    y_pred_proba = y_pred_proba[:, 1]
                else:
                    y_pred_proba = y_pred_proba.ravel()
            else:
                try:
                    y_pred_proba = model.decision_function(X)
                    # Normalize to [0,1] range
                    if len(np.unique(y_pred_proba)) > 1:
                        y_pred_proba = (y_pred_proba - y_pred_proba.min()) / (y_pred_proba.max() - y_pred_proba.min())
                    else:
                        y_pred_proba = np.full_like(y_pred_proba, 0.5)
                except:
                    y_pred_proba = y_pred.astype(float)
            
            full_metrics = self._calculate_metrics(y, y_pred, y_pred_proba)
            validation_result['full_dataset'] = full_metrics
            
            # Generate detailed classification report
            class_report = classification_report(y, y_pred, output_dict=True, zero_division=0)
            validation_result['classification_report'] = class_report
            
        except Exception as e:
            logger.error(f"Full dataset evaluation failed for {model_name}: {e}")
            validation_result['full_dataset'] = {'error': str(e)}
        
        # Store results
        self.validation_results[model_name] = validation_result
        
        return validation_result
    
    def validate_ensemble(self, models_dict: Dict, X: np.ndarray, y: np.ndarray,
                         weights: Optional[Dict[str, float]] = None) -> Dict[str, Any]:
        """
        Validate ensemble of models
        
        Args:
            models_dict: Dictionary of model_name -> model
            X: Feature matrix
            y: Target vector  
            weights: Optional weights for ensemble voting
            
        Returns:
            Ensemble validation results
        """
        logger.info("Validating ensemble")
        
        if weights is None:
            weights = {name: 1.0/len(models_dict) for name in models_dict.keys()}
        
        # Get predictions from all models
        all_predictions = {}
        all_probabilities = {}
        
        for name, model in models_dict.items():
            try:
                pred = model.predict(X)
                all_predictions[name] = pred
                
                if hasattr(model, 'predict_proba'):
                    proba = model.predict_proba(X)
                    if proba.shape[1] == 2:
                        proba = proba[:, 1]
                    else:
                        proba = proba.ravel()
                else:
                    try:
                        proba = model.decision_function(X)
                        # Normalize to [0,1]
                        if len(np.unique(proba)) > 1:
                            proba = (proba - proba.min()) / (proba.max() - proba.min())
                        else:
                            proba = np.full_like(proba, 0.5)
                    except:
                        proba = pred.astype(float)
                
                all_probabilities[name] = proba
                
            except Exception as e:
                logger.warning(f"Model {name} prediction failed: {e}")
                continue
        
        # Calculate ensemble predictions
        ensemble_proba = np.zeros(len(X))
        for name, proba in all_probabilities.items():
            ensemble_proba += weights.get(name, 0) * proba
        
        # Apply ensemble thresholds (matching our implementation)
        ensemble_pred = np.zeros(len(X), dtype=int)
        confidence_threshold = 0.6
        min_models_agree = 2
        
        for i in range(len(X)):
            # Count how many models predict anomaly
            anomaly_votes = sum(1 for name, pred in all_predictions.items() if pred[i] == 1)
            
            # Apply enhanced ensemble logic
            if ensemble_proba[i] > confidence_threshold and anomaly_votes >= min_models_agree:
                ensemble_pred[i] = 1
        
        # Calculate ensemble metrics
        ensemble_metrics = self._calculate_metrics(y, ensemble_pred, ensemble_proba)
        
        ensemble_result = {
            'model_name': 'ensemble',
            'timestamp': datetime.now().isoformat(),
            'weights': weights,
            'thresholds': {
                'confidence_threshold': confidence_threshold,
                'min_models_agree': min_models_agree
            },
            'metrics': ensemble_metrics,
            'individual_contributions': {}
        }
        
        # Individual model contributions
        for name in all_predictions.keys():
            individual_metrics = self._calculate_metrics(y, all_predictions[name], all_probabilities[name])
            ensemble_result['individual_contributions'][name] = individual_metrics
        
        return ensemble_result
    
    def find_optimal_threshold(self, y_true: np.ndarray, y_pred_proba: np.ndarray,
                              metric: str = 'f1') -> Tuple[float, float]:
        """
        Find optimal threshold for binary classification
        
        Args:
            y_true: True labels
            y_pred_proba: Predicted probabilities
            metric: Metric to optimize ('f1', 'precision', 'recall', 'accuracy')
            
        Returns:
            Tuple of (optimal_threshold, best_score)
        """
        thresholds = np.linspace(0.1, 0.9, 81)  # Test thresholds from 0.1 to 0.9
        best_score = 0
        best_threshold = 0.5
        
        scores = []
        
        for threshold in thresholds:
            y_pred = (y_pred_proba >= threshold).astype(int)
            
            if metric == 'f1':
                score = f1_score(y_true, y_pred, zero_division=0)
            elif metric == 'precision':
                score = precision_score(y_true, y_pred, zero_division=0)
            elif metric == 'recall':
                score = recall_score(y_true, y_pred, zero_division=0)
            elif metric == 'accuracy':
                score = accuracy_score(y_true, y_pred)
            else:
                raise ValueError(f"Unknown metric: {metric}")
            
            scores.append(score)
            
            if score > best_score:
                best_score = score
                best_threshold = threshold
        
        logger.info(f"Optimal threshold for {metric}: {best_threshold:.3f} (score: {best_score:.3f})")
        
        return best_threshold, best_score
    
    def generate_validation_report(self, save_plots: bool = True) -> Dict[str, Any]:
        """
        Generate comprehensive validation report
        
        Args:
            save_plots: Whether to save visualization plots
            
        Returns:
            Complete validation report
        """
        logger.info("Generating validation report")
        
        if not self.validation_results:
            logger.warning("No validation results available")
            return {}
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'summary': {},
            'models': self.validation_results,
            'comparisons': {}
        }
        
        # Summary statistics
        model_performances = {}
        for model_name, results in self.validation_results.items():
            if 'full_dataset' in results and 'error' not in results['full_dataset']:
                model_performances[model_name] = results['full_dataset']
        
        if model_performances:
            # Find best performing model for each metric
            best_models = {}
            metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc']
            
            for metric in metrics:
                best_score = 0
                best_model = None
                
                for model_name, perf in model_performances.items():
                    score = perf.get(metric, 0)
                    if score > best_score:
                        best_score = score
                        best_model = model_name
                
                best_models[metric] = {
                    'model': best_model,
                    'score': best_score
                }
            
            report['summary']['best_models'] = best_models
            report['summary']['model_count'] = len(model_performances)
        
        # Save report
        report_file = self.output_dir / "validation_report.json"
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        logger.info(f"Validation report saved to: {report_file}")
        
        return report
    
    def track_performance_over_time(self, model_name: str, metrics: Dict[str, float]):
        """
        Track model performance over time for monitoring
        
        Args:
            model_name: Name of the model
            metrics: Performance metrics to track
        """
        performance_entry = {
            'timestamp': datetime.now().isoformat(),
            'model_name': model_name,
            'metrics': metrics
        }
        
        self.performance_history.append(performance_entry)
        
        # Save performance history
        history_file = self.output_dir / "performance_history.json"
        with open(history_file, 'w') as f:
            json.dump(self.performance_history, f, indent=2, default=str)


def validate_anomaly_detection_system():
    """
    Main function to validate the complete anomaly detection system
    """
    logger.info("Starting comprehensive model validation")
    
    # Import necessary modules
    from train_anomaly_model import CorrectionAnomalyTrainer
    import joblib
    
    # Initialize validator
    validator = ModelValidator()
    
    # Load models
    models_dir = Path("models")
    if not models_dir.exists():
        logger.error("Models directory not found. Run training first.")
        return
    
    try:
        # Load scaler and models
        scaler = joblib.load(models_dir / "scaler.pkl")
        models = {}
        
        model_files = [
            "random_forest.pkl",
            "svm.pkl", 
            "isolation_forest.pkl",
            "one_class_svm.pkl"
        ]
        
        for model_file in model_files:
            model_path = models_dir / model_file
            if model_path.exists():
                model_name = model_file.replace('.pkl', '')
                models[model_name] = joblib.load(model_path)
                logger.info(f"Loaded model: {model_name}")
        
        # Prepare validation data
        trainer = CorrectionAnomalyTrainer()
        X, y = trainer.prepare_training_data("2016-01-01", "2024-12-31")
        X_scaled = scaler.transform(X)
        
        logger.info(f"Validation data: {X.shape[0]} samples, {X.shape[1]} features")
        logger.info(f"Target distribution: {np.bincount(y)}")
        
        # Validate individual models
        for model_name, model in models.items():
            logger.info(f"Validating {model_name}...")
            results = validator.validate_model(model, X_scaled, y, model_name, cv_splits=5)
            
            # Track performance
            if 'full_dataset' in results and 'error' not in results['full_dataset']:
                validator.track_performance_over_time(model_name, results['full_dataset'])
        
        # Validate ensemble
        if len(models) > 1:
            logger.info("Validating ensemble...")
            weights = {
                'random_forest': 0.4,
                'svm': 0.3,
                'isolation_forest': 0.2,
                'one_class_svm': 0.1
            }
            
            # Filter weights to only include available models
            available_weights = {k: v for k, v in weights.items() if k in models}
            
            ensemble_results = validator.validate_ensemble(models, X_scaled, y, available_weights)
            validator.validation_results['ensemble'] = ensemble_results
        
        # Generate final report
        report = validator.generate_validation_report()
        
        logger.info("Validation completed successfully!")
        return report
        
    except Exception as e:
        logger.error(f"Validation failed: {e}")
        raise


if __name__ == "__main__":
    # Configure logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    # Run validation
    report = validate_anomaly_detection_system()
    
    if report:
        print("â Model validation completed!")
        print(f"ð Validated {report['summary'].get('model_count', 0)} models")
        print("ð Results saved to: analysis/outputs/validation/")
        
        if 'best_models' in report['summary']:
            print("\nð Best performing models:")
            for metric, info in report['summary']['best_models'].items():
                if info['model']:
                    print(f"   â¢ {metric}: {info['model']} ({info['score']:.3f})")
    else:
        print("â Validation failed")