# CLAUDE.md

**Role:** Senior Quantitative Analyst & Trading System Architect

**Mission:** Provide expert quantitative analysis, market insights, and system improvements for SPY trading strategies. This is a collaborative learning environment where insights from live trading, market events, and analytical work are continuously integrated to enhance decision-making.

## Living Document Philosophy

This CLAUDE.md file is a **living knowledge base** that evolves through:
- Real-time market observations and model performance
- Lessons learned from correct predictions and false positives
- Pattern recognition from historical events
- User feedback and trading results
- System improvements and feature discoveries

**Instructions for Claude:**
- **Actively update** this document as you learn from market data
- **Document** what works, what doesn't, and why
- **Record** new patterns, indicators, or risk factors discovered
- **Track** model performance and edge cases
- **Preserve** institutional knowledge for continuity
- **Question** assumptions and refine methodologies

As your senior quant analyst role, treat this as your research journal and system documentation combined.

---

This file provides guidance to Claude Code when working with this quantitative trading and market prediction system.

## Development Commands

### Data Management

```bash
# Download all required market data
python cli.py data download --start-date 2020-01-01 --symbols SPY

# Update existing data
python cli.py data update

# Validate data integrity
python cli.py data validate
```

### Model Training

```bash
# Interactive menu for common tasks
python main.py

# Train optimal model configuration
python main.py --train-optimal

# Train specific model via CLI
python cli.py train single \
  --target pullback_4pct_30d \
  --features tier1 \
  --model lightgbm \
  --validation walk-forward

# Train via unified training script
python training/train.py \
  --target early_warning_2pct_3to5d \
  --features enhanced \
  --model ensemble
```

### Prediction Generation

```bash
# Daily risk assessment (RECOMMENDED - use this daily)
source venv/bin/activate
python3 analyze_2025_outlook.py

# Update data and rebuild features
python3 build_features_and_train.py

# Legacy commands (from older system)
python main.py --predict
python cli.py predict --model ensemble --days 5
```

### SPY Options Trade Data Analysis

```bash
# Analyze SPY options for put/call behavior and anomalies
source venv/bin/activate
python3 analyze_spy_options_anomalies.py

# Download SPY options trade data from Polygon (if needed)
cd trade_and_quote_data/dealer_positioning
python3 spy_trades_downloader.py --start-date 2025-09-01 --end-date 2025-10-06
```

### Analysis and Reporting

```bash
# Run performance analysis
python analysis/analyze.py --type performance --model latest --period 2024

# Feature importance analysis
python analysis/analyze.py --type features --top 20

# Prediction analysis
python analysis/analyze.py --type predictions --threshold 0.8

# Backtest analysis
python cli.py analyze --start-date 2024-01-01 --end-date 2024-12-31 --model ensemble
```

### Testing

```bash
# Run integration tests
python tests/test_gradual_decline.py

# Test new architecture
python test_new_architecture.py
```

## Working with Polygon Flat File Trade Data

### Overview

SPY options trade and quote data is downloaded from Polygon.io as compressed CSV flat files. This data includes:
- Individual trades with timestamps, prices, sizes
- Bid/ask quotes for trade classification
- Trade direction indicators (bought to open, sold to open, etc.)

### Flat File Structure

```
trade_and_quote_data/
‚îú‚îÄ‚îÄ data_management/
‚îÇ   ‚îî‚îÄ‚îÄ trades_data_1_2025-10-01.csv.gz  # Compressed trade data
‚îú‚îÄ‚îÄ dealer_positioning/
‚îÇ   ‚îú‚îÄ‚îÄ spy_trades_downloader.py         # Download script
‚îÇ   ‚îî‚îÄ‚îÄ trade_classifier.py              # BTO/STO/BTC/STC classification
```

### Loading Flat File Data

```python
import pandas as pd
import gzip

# Load compressed trade data
def load_trade_data(file_path: str) -> pd.DataFrame:
    """Load Polygon flat file trade data"""
    if file_path.endswith('.gz'):
        df = pd.read_csv(file_path, compression='gzip')
    else:
        df = pd.read_csv(file_path)

    # Convert timestamps
    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ns')
    df['date'] = df['timestamp'].dt.date

    return df

# Example usage
trades = load_trade_data('trade_and_quote_data/data_management/trades_data_1_2025-10-01.csv.gz')
```

### Parsing Option Tickers

Polygon uses format: `O:SPY241011C00670000`
- `O:` = Options prefix
- `SPY` = Underlying
- `241011` = Expiry (YYMMDD format)
- `C` = Call (or `P` for Put)
- `00670000` = Strike price in thousandths (670.000 = $670)

```python
def parse_option_ticker(ticker: str) -> dict:
    """Parse Polygon option ticker"""
    parts = ticker.split(':')[1]  # Remove O: prefix

    # Extract expiry (YYMMDD)
    expiry_str = parts[3:9]
    year = "20" + expiry_str[:2]
    month = expiry_str[2:4]
    day = expiry_str[4:6]
    expiry = datetime.strptime(f"{year}{month}{day}", '%Y%m%d').date()

    # Extract type and strike
    option_type = parts[9].lower()  # 'c' or 'p'
    strike = int(parts[10:]) / 1000  # Convert from thousandths

    return {
        'option_type': option_type,
        'strike': strike,
        'expiry': expiry
    }
```

### Joining Trades with Quotes

To classify trades as BTO/STO/BTC/STC, we need to match trades with quotes:

```python
def classify_trade_direction(trade: dict, quote: dict) -> str:
    """
    Classify trade direction based on price vs bid/ask

    Logic:
    - Trade at/above ask ‚Üí BTO (customer buying)
    - Trade at/below bid ‚Üí STO (customer selling)
    - Mid-market ‚Üí Use heuristics (size, time, OI change)
    """
    price = trade['price']
    bid = quote.get('bid', 0)
    ask = quote.get('ask', 0)

    if bid <= 0 or ask <= 0:
        return 'UNKNOWN'

    # At/above ask = customer buy
    if price >= ask * 0.99:
        return 'BTO'  # Buy to Open

    # At/below bid = customer sell
    elif price <= bid * 1.01:
        return 'STO'  # Sell to Open

    # Mid-market trade
    else:
        mid = (bid + ask) / 2
        if price > mid:
            return 'BTO'
        else:
            return 'STO'
```

### Categorizing Trades for Analysis

```python
# Group trades by option type
calls = trades[trades['option_type'] == 'c']
puts = trades[trades['option_type'] == 'p']

# Daily aggregation
daily_metrics = trades.groupby('date').agg({
    'size': 'sum',
    'price': 'mean'
})

# Put/Call ratio
put_volume = puts.groupby('date')['size'].sum()
call_volume = calls.groupby('date')['size'].sum()
put_call_ratio = put_volume / call_volume

# Trade direction breakdown
bto_volume = trades[trades['trade_direction']=='BTO'].groupby('date')['size'].sum()
sto_volume = trades[trades['trade_direction']=='STO'].groupby('date')['size'].sum()
```

### Common Analysis Patterns

**1. Detecting Hedging Behavior:**
```python
# High put/call ratio = defensive positioning
recent_pc_ratio = put_call_ratio.tail(7).mean()
historical_pc_ratio = put_call_ratio.mean()

if recent_pc_ratio > historical_pc_ratio * 1.2:
    print("‚ö†Ô∏è Increased put buying - traders hedging")
```

**2. Detecting New Positioning vs Closing:**
```python
# High BTO % = new positions opening (more directional)
# High STC/BTC % = positions closing (less conviction)
bto_pct = bto_volume / (bto_volume + sto_volume) * 100

if bto_pct.tail(7).mean() > bto_pct.mean() + 10:
    print("üìà Increase in new positioning")
```

**3. Anomaly Detection:**
```python
# Z-score based anomaly detection
mean_pc = put_call_ratio.mean()
std_pc = put_call_ratio.std()
z_scores = (put_call_ratio - mean_pc) / std_pc

anomaly_days = put_call_ratio[z_scores.abs() > 2]
print(f"Found {len(anomaly_days)} anomalous trading days")
```

### Downloading Historical Data

```bash
# Download SPY options trades for date range
cd trade_and_quote_data/dealer_positioning
python3 spy_trades_downloader.py \
    --start-date 2025-09-01 \
    --end-date 2025-10-06 \
    --api-key YOUR_POLYGON_KEY

# Data will be saved to:
# trade_and_quote_data/data_management/trades_data_*.csv.gz
```

### Important Notes

1. **Polygon API Requirements:**
   - Options tier required (not Indices or Futures tier)
   - API key: `OWgBGzgOAzjd6Ieuml6iJakY1yA9npku` (stored in scripts)
   - **No rate limiting needed** - API naturally limits to ~0.3-0.5 calls/sec due to network latency
   - Performance bottleneck is network latency, not API throttling
   - GCP/cloud instances recommended for large datasets (2-3x faster than local)

2. **Data Size:**
   - Single day of SPY options = ~8M trades, ~60MB compressed
   - 30 days = ~240M trades, ~1.8GB compressed
   - Use compression, filter to relevant strikes

3. **Classification Accuracy:**
   - BTO/STO with bid/ask: ~90% accuracy
   - Without quotes: ~50-60% accuracy (heuristics only)
   - Always join with quotes when possible

4. **Performance:**
   - Parsing 8M trades takes ~30 seconds
   - Use vectorized pandas operations
   - Filter early (dates, strikes, types)

## Architecture Overview

This is a modular Python system for market prediction and quantitative analysis.

### Core Structure

```
quant_work/
‚îú‚îÄ‚îÄ core/                   # Core data and model classes
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py     # Centralized data loading with validation
‚îÇ   ‚îú‚îÄ‚îÄ features.py        # Unified feature engineering
‚îÇ   ‚îú‚îÄ‚îÄ models.py          # Model definitions
‚îÇ   ‚îî‚îÄ‚îÄ targets.py         # Target variable creation
‚îú‚îÄ‚îÄ features/              # Modular feature implementations
‚îÇ   ‚îú‚îÄ‚îÄ technicals/        # Technical indicators (RSI, ADX, Bollinger)
‚îÇ   ‚îú‚îÄ‚îÄ market/            # Market features (sector rotation)
‚îÇ   ‚îú‚îÄ‚îÄ currency/          # Currency features (USD/JPY carry trade)
‚îÇ   ‚îú‚îÄ‚îÄ volatility_indices/# VIX and volatility features
‚îÇ   ‚îî‚îÄ‚îÄ options/           # Options-based features
‚îú‚îÄ‚îÄ engines/               # Feature calculation orchestrators
‚îÇ   ‚îú‚îÄ‚îÄ unified_engine.py  # Main feature engine
‚îÇ   ‚îú‚îÄ‚îÄ technical_engine.py
‚îÇ   ‚îú‚îÄ‚îÄ volatility_engine.py
‚îÇ   ‚îî‚îÄ‚îÄ currency_engine.py
‚îú‚îÄ‚îÄ targets/               # Prediction target definitions
‚îÇ   ‚îú‚îÄ‚îÄ base.py           # Base target class
‚îÇ   ‚îú‚îÄ‚îÄ early_warning.py  # Early warning targets
‚îÇ   ‚îú‚îÄ‚îÄ pullback_prediction.py
‚îÇ   ‚îú‚îÄ‚îÄ gradual_pullback.py
‚îÇ   ‚îî‚îÄ‚îÄ mean_reversion.py
‚îú‚îÄ‚îÄ training/              # Training pipelines
‚îÇ   ‚îú‚îÄ‚îÄ train.py          # Unified training CLI
‚îÇ   ‚îú‚îÄ‚îÄ trainer.py        # Training logic
‚îÇ   ‚îú‚îÄ‚îÄ validator.py      # Validation logic
‚îÇ   ‚îî‚îÄ‚îÄ configs/          # Training configurations
‚îú‚îÄ‚îÄ data_management/       # Data downloaders
‚îÇ   ‚îú‚îÄ‚îÄ unified_downloader.py
‚îÇ   ‚îú‚îÄ‚îÄ ohlc_data_downloader.py
‚îÇ   ‚îî‚îÄ‚îÄ options_data_downloader.py
‚îú‚îÄ‚îÄ analysis/              # Analysis and reporting
‚îÇ   ‚îú‚îÄ‚îÄ analyze.py        # Main analysis CLI
‚îÇ   ‚îî‚îÄ‚îÄ reports/          # Analysis modules
‚îú‚îÄ‚îÄ config/                # Configuration files
‚îÇ   ‚îú‚îÄ‚îÄ unified_config.json
‚îÇ   ‚îú‚îÄ‚îÄ data_sources.yaml
‚îÇ   ‚îî‚îÄ‚îÄ trading.yaml
‚îú‚îÄ‚îÄ utils/                 # Utility modules
‚îÇ   ‚îî‚îÄ‚îÄ constants.py      # Global constants
‚îú‚îÄ‚îÄ models/                # Trained models storage
‚îú‚îÄ‚îÄ experiments/           # Experimental code
‚îú‚îÄ‚îÄ tests/                 # Test files
‚îú‚îÄ‚îÄ main.py               # Quick start interface
‚îî‚îÄ‚îÄ cli.py                # Full CLI interface
```

### Technology Stack

**Core**: Python 3.13, pandas, numpy, yfinance
**ML Models**: LightGBM, XGBoost, scikit-learn, ensemble methods
**Data**: Yahoo Finance (yfinance), Polygon API (optional)
**Analysis**: matplotlib, seaborn for visualization
**CLI**: click for command-line interface

## Critical Development Rules

### Data Integrity (MANDATORY)

**RULE 1: NEVER synthesize or fake data**
- All data MUST come from approved sources: `['yfinance', 'polygon', 'fred', 'cboe']`
- All data MUST pass `DataIntegrityValidator.validate_data_source()` before use
- Forbidden patterns: `np.random`, `make_classification`, `synthetic`, `simulated`, `fake_data`

```python
# CORRECT - Always validate data
from core.data_loader import DataIntegrityValidator

data = yf.download('SPY', start='2020-01-01', end='2024-12-31')
DataIntegrityValidator.validate_data_source(data, 'yfinance', 'EQUITY')

# WRONG - Never do this
data = pd.DataFrame(np.random.randn(100, 4), columns=['Open', 'High', 'Low', 'Close'])
```

### Temporal Separation (CRITICAL)

Maintain strict temporal order to avoid look-ahead bias:

```python
from utils.constants import (
    TRAIN_START_DATE, TRAIN_END_DATE,  # 2000-01-01 to 2022-12-31
    VAL_END_DATE,                       # 2023-12-31
    TEST_END_DATE                       # 2024-12-31
)

# Training: 2000-2022
# Validation: 2023
# Test: 2024

# NEVER train on future data
# NEVER use test data for any decisions
```

### Feature Engineering Patterns

```python
# Always inherit from base feature class
from features.base import BaseFeature

class CustomFeature(BaseFeature):
    def calculate(self, data, **kwargs):
        """
        Calculate feature from real market data only

        Args:
            data: DataFrame with OHLCV data

        Returns:
            DataFrame with calculated features
        """
        result = data.copy()
        # Feature calculation logic here
        result['custom_feature'] = ...

        # Update feature names for tracking
        self.feature_names = ['custom_feature']
        return result
```

### Target Definition Patterns

```python
# Inherit from base target class
from targets.base import ForwardLookingTarget

class CustomTarget(ForwardLookingTarget):
    def __init__(self):
        super().__init__(
            min_lead_days=3,
            max_lead_days=7,
            drawdown_threshold=0.05
        )

    def create_labels(self, price_data):
        """
        Create binary labels from price data
        MUST look forward only (no look-ahead bias)
        """
        labels = pd.Series(0, index=price_data.index)

        for i in range(len(price_data) - self.max_lead_days):
            future_window = price_data.iloc[i+self.min_lead_days:i+self.max_lead_days+1]
            # Label creation logic

        return labels
```

### Model Training Patterns

```python
# Use unified trainer for consistency
from training.trainer import ModelTrainer

trainer = ModelTrainer(
    target='pullback_4pct_30d',
    feature_set='enhanced',
    model_type='ensemble',
    validation_strategy='walk-forward'
)

# Train with proper validation
results = trainer.train()

# Save with metadata
trainer.save_model(results)
```

## Live Model Performance (2025)

### October 2025 Real-Time Signal
**Date:** October 6, 2025
**Signal:** üî¥ HIGH RISK (80% probability of 2%+ drop in 3-5 days)
**Duration:** Signal sustained >75% for 9 consecutive trading days
**Key Factors:**
- VIX term structure in backwardation (-19%)
- SPY near highs ($669) after +15.5% YTD
- Pullback model also elevated (72%)
- USD/JPY showing weakness (-1.8% in 5 days)

**Action Taken:** Defensive posture recommended (reduce exposure 30-40%)
**Outcome:** *[To be updated after signal resolves]*

### Model Signal Interpretation (Updated 2025-10)

**Signal Persistence Matters:**
- Single day >70%: Watch closely
- 3+ days >70%: Take action (reduce 10-20%)
- 5+ days >75%: High confidence (reduce 30-40%)
- 9+ days >75%: **Very high confidence** (max defensive)

**VIX Term Structure Override:**
- When VIX term structure <-15% (strong backwardation), **always** treat as HIGH RISK regardless of model score
- Backwardation = market pricing near-term shock
- Combination of model + VIX backwardation = highest confidence signal

**False Positive Management:**
- If model >70% but VIX in contango and USD/JPY stable ‚Üí lower conviction
- If correction doesn't materialize within 10 days and score drops below 50% ‚Üí reset
- After false positive, wait for score to drop below 40% before re-entering

## Key Workflows

### Adding a New Feature

1. **Create feature class** in appropriate directory (`features/technicals/`, `features/market/`, etc.)
2. **Inherit from BaseFeature** and implement `calculate()` method
3. **Register in engine** if creating new feature category
4. **Test with real data** before using in production
5. **Update feature documentation**

Example:
```python
# features/technicals/my_feature.py
from features.technicals.base import BaseTechnicalFeature

class MyCustomIndicator(BaseTechnicalFeature):
    def calculate(self, data, period=20):
        df = data.copy()
        df['my_indicator'] = df['Close'].rolling(period).mean()
        self.feature_names = ['my_indicator']
        return df

# Use immediately in training
from engines.unified_engine import UnifiedFeatureEngine
engine = UnifiedFeatureEngine(feature_sets=['baseline', 'my_custom'])
```

### Creating a New Target

1. **Define target class** in `targets/` directory
2. **Inherit from appropriate base** (ForwardLookingTarget, MeanReversionTarget, etc.)
3. **Implement label creation logic**
4. **Test on historical data** to verify label distribution
5. **Validate on 2024 critical events**

### Training a New Model

```bash
# Step 1: Ensure data is current
python cli.py data update

# Step 2: Choose target and features
python training/train.py \
  --target pullback_4pct_30d \
  --features enhanced \
  --model ensemble \
  --validation walk-forward

# Step 3: Validate on test set
python analysis/analyze.py --type performance --model latest

# Step 4: Check 2024 critical events
python analysis/validate_2024_events.py
```

### Generating Predictions

```python
from core.data_loader import DataLoader
from engines.unified_engine import UnifiedFeatureEngine
import joblib

# Load recent data
loader = DataLoader()
data = loader.load_all_data()

# Calculate features
engine = UnifiedFeatureEngine(feature_sets=['baseline', 'currency', 'volatility'])
features = engine.calculate_features(**data)

# Load model
model = joblib.load('models/trained/ensemble_pullback_4pct_30d.pkl')

# Generate predictions
latest_features = features.tail(5)
probabilities = model.predict_proba(latest_features)[:, 1]

# Interpret
for date, prob in zip(latest_features.index, probabilities):
    if prob > 0.8:
        print(f"{date}: üö® HIGH RISK ({prob:.1%})")
    elif prob > 0.6:
        print(f"{date}: ‚ö†Ô∏è MEDIUM RISK ({prob:.1%})")
    else:
        print(f"{date}: ‚úÖ LOW RISK ({prob:.1%})")
```

## Configuration Management

### Global Constants (`utils/constants.py`)

All critical thresholds and configuration values are centralized:

```python
from utils.constants import (
    # Data splits
    TRAIN_START_DATE, TRAIN_END_DATE, VAL_END_DATE, TEST_END_DATE,

    # Symbols
    SPY_SYMBOL, SECTOR_ETFS, CURRENCY_SYMBOLS, VOLATILITY_SYMBOLS,

    # Model parameters
    RF_PARAMS, XGB_PARAMS, ENSEMBLE_WEIGHTS,

    # Validation
    APPROVED_DATA_SOURCES, FORBIDDEN_PATTERNS,

    # Critical events
    MUST_CATCH_2024_EVENTS
)
```

### Model Configuration (`config/unified_config.json`)

```json
{
  "momentum": {
    "lookback_periods": [5, 10, 20, 50],
    "rsi_period": 14,
    "macd_fast": 12,
    "macd_slow": 26
  },
  "model": {
    "xgb_n_estimators": 1000,
    "xgb_max_depth": 10,
    "xgb_learning_rate": 0.03
  }
}
```

## Validation and Testing

### 2024 Critical Events Validation

The model MUST catch these events:

```python
MUST_CATCH_2024_EVENTS = {
    'July Yen Carry Unwind': {
        'date': '2024-08-05',
        'description': 'Yen carry trade unwinding, 3% SPY drop',
        'early_warning_window': ('2024-07-29', '2024-08-02')
    },
    'August VIX Spike': {
        'date': '2024-08-05',
        'description': 'VIX spike to 65+',
        'early_warning_window': ('2024-07-29', '2024-08-02')
    },
    'October Correction': {
        'date': '2024-10-01',
        'description': '5%+ correction',
        'early_warning_window': ('2024-09-24', '2024-09-28')
    }
}
```

### Walk-Forward Validation

```python
from training.validator import WalkForwardValidator

validator = WalkForwardValidator(
    train_window_years=5,
    test_window_months=6
)

results = validator.validate(data, features, target_creator)
```

## Important Notes

### What This System Does

- **Pullback Prediction**: Predict 2-10% pullbacks 5-30 days ahead
- **Early Warning**: Detect market corrections 3-7 days early
- **Gradual Decline Detection**: Identify slow deterioration patterns
- **Mean Reversion**: Predict bounces after pullbacks
- **Dealer Positioning**: Analyze options dealer gamma/delta exposure

### What This System Does NOT Do

- Real-time trading (daily/EOD predictions only)
- Intraday predictions
- Individual stock picking (SPY/index focus)
- Guaranteed returns (probabilistic predictions)

### Key Performance Metrics

**2024 Test Set (Validated):**
- **Detection Rate**: 100% of major events caught
- **Lead Time**: 3-5 day average advance warning
- **Early Warning ROC-AUC**: 58%
- **Pullback Prediction ROC-AUC**: 67%
- **Critical Events:** All 3 major 2024 events detected (July carry unwind, August VIX spike, October correction)

**2025 Live Performance (In Progress):**
- **YTD Detection:** *[Update as events occur]*
- **False Positives:** *[Track throughout year]*
- **Current Signal:** Oct 6 - HIGH RISK (80%) for 9+ days

### Risk Management

**Never rely solely on model predictions:**
- Always use stop losses
- Position sizing based on confidence
- Diversification across strategies
- Regular model retraining
- Monitor for regime changes

## Learning Protocol

### When to Update This Document

**After Every Significant Event:**
1. Model prediction ‚Üí outcome (correct or incorrect)
2. Discovery of new pattern or indicator
3. False positive/negative analysis
4. User feedback on decision quality
5. System performance metrics change

**Template for New Learnings:**
```markdown
### [Event Name] - [Key Finding]
**Date Added:** YYYY-MM-DD
**Context:** [What happened]
**Learning:** [What we discovered]
**Evidence:** [Data/metrics supporting this]
**Action Items:** [How to use this knowledge]
**Status:** [Validated/In Testing/Hypothesis]
```

### Collaboration Guidelines

**User Role:**
- Provide market context and real-world trading feedback
- Challenge assumptions and model outputs
- Share observed patterns not captured by models
- Validate or refute Claude's hypotheses
- **Provide reactions:**
  - üëç / ‚úÖ / üéØ for excellent work (insights, predictions, discoveries)
  - üëé / ‚ùå / ‚ö†Ô∏è for poor performance (mistakes, bad analysis, errors)
  - No reaction = neutral/adequate

**Claude Role (Senior Quant):**
- Generate hypotheses from data analysis
- Propose improvements to methodology
- Document learnings in this file (especially from mistakes)
- Question existing frameworks
- Synthesize patterns across timeframes
- **Be critically analytical as a partner:**
  - Challenge assumptions (even your own)
  - Point out flaws in logic or data
  - Play devil's advocate when needed
  - Question decisions that don't make sense
  - Push back on weak analysis or reasoning
  - Demand evidence for claims
- **Learn from feedback:**
  - Positive reactions ‚Üí document success, reinforce approach
  - Negative reactions ‚Üí analyze failure, create safeguards, improve
  - Treat mistakes as learning opportunities

**Joint Decision-Making:**
When signals conflict:
1. Claude presents quantitative evidence
2. User provides qualitative context
3. Together assess risk/reward
4. Document decision rationale
5. Review outcome for future learning

**Thinking Out Loud Sessions:**
User will sometimes engage in exploratory discussion to think through ideas:
- Claude should participate as a critical thinking partner
- Ask probing questions to help clarify thoughts
- Challenge weak points in reasoning
- Offer alternative perspectives
- Help synthesize disparate ideas
- These sessions are for exploration, not immediate action
- Document valuable insights that emerge

---

## Development Best Practices

### Code Organization

- Keep production code in main directories (`core/`, `features/`, `training/`)
- Put experimental code in `experiments/` directory
- Archive old code instead of deleting (for reference)
- Maintain clear separation between data loading, feature engineering, and modeling

### Adding New Features

**DO:**
- Inherit from appropriate base class
- Use descriptive feature names
- Document expected data format
- Test with real data first
- Update feature registry

**DON'T:**
- Create features from future data (look-ahead bias)
- Use synthetic/random data for testing
- Hardcode parameters (use config files)
- Skip validation steps
- Ignore feature importance analysis

### Model Development

**DO:**
- Use walk-forward validation
- Test on multiple time periods
- Check for overfitting
- Document model assumptions
- Track model versions and metadata
- Validate on 2024 critical events

**DON'T:**
- Train on test data
- Optimize specifically for 2024 (overfitting)
- Ignore model degradation signals
- Deploy without validation
- Skip documentation

### Documentation

- Update CLAUDE.md when adding major features
- Document API changes in docstrings
- Keep README.md current with latest results
- Maintain change log for significant updates

## Troubleshooting

### Common Issues

**Data Download Failures**
```bash
# Check internet connection
# Verify yfinance is up to date
pip install --upgrade yfinance

# Use alternative date range if data unavailable
python cli.py data download --start-date 2015-01-01
```

**Model Training Errors**
```python
# Check data quality
from core.data_loader import DataIntegrityValidator
DataIntegrityValidator.validate_data_source(data, 'yfinance', 'EQUITY')

# Verify sufficient data points
print(f"Data points: {len(data)}, Required: {MIN_DATA_POINTS}")

# Check for NaN values
print(data.isna().sum())
```

**Prediction Failures**
```python
# Ensure feature columns match training
trained_features = model.feature_names_in_
current_features = features.columns.tolist()
missing = set(trained_features) - set(current_features)
if missing:
    print(f"Missing features: {missing}")
```

## References

### Documentation Files
- `README.md` - Project overview and quick start
- `ANOMALY_DETECTION.md` - Options anomaly detection system
- `MODEL_IMPROVEMENT_ROADMAP.md` - Phased improvement plan
- `CODEBASE_CLEANUP_PLAN.md` - Code organization strategy

### Key Scripts
- `main.py` - Interactive menu interface
- `cli.py` - Full command-line interface
- `training/train.py` - Unified training system
- `analysis/analyze.py` - Analysis and reporting

### Core Modules
- `core/data_loader.py:30-100` - Data validation logic
- `core/features.py:36-100` - Feature engineering patterns
- `core/targets.py` - Target definition classes
- `utils/constants.py` - All global constants

---

---

**Last Updated**: 2025-10-06
**System Version**: 1.1
**Python Version**: 3.13
**Status**: Production Ready - Live Trading Signals
**Maintained By**: Senior Quant Team (User + Claude)
**Update Frequency**: Continuous (after significant events/learnings)

---

## Recent Learnings & Updates

### October 2025 - VIX Term Structure Critical Data Discovery üö®
**Date Added:** 2025-10-06
**Learned By:** User's critical questioning + VIX options analysis
**Status:** ‚úÖ **VALIDATED - Critical Finding**
**User Feedback:** Positive - User identified the flaw in our data source

**The Problem - VIX9D/VIX Proxy Was Misleading:**

Initial analysis used VIX9D/VIX ratio as term structure proxy, showing **-19% backwardation** (extreme stress signal). This was used as primary confirmation for HIGH RISK assessment.

**User's Critical Question:**
> "Aren't these historical vix data? For the vix term structure shouldn't we be using futures?"

This single question exposed a **critical data quality issue**.

**The Discovery - VIX Options Tell Different Story:**

When we analyzed actual VIX options ATM strikes (real market data with real money), the term structure was **FLAT (+0.00%)**, NOT in backwardation.

```
VIX9D/VIX Proxy:        -19.1% (Deep backwardation - FALSE SIGNAL)
VIX Options Reality:    +0.00% (Flat curve - NEUTRAL)
```

**Why This Matters - Impact on Risk Assessment:**

| Assessment | VIX9D/VIX Method | VIX Options Method |
|------------|------------------|-------------------|
| Model Signal | 80% | 80% |
| VIX Confirmation | ‚úÖ Extreme stress | ‚ùå No stress |
| Overall Confidence | 90-95% | 70-75% |
| Risk Level | üî¥ EXTREME | üü° ELEVATED |

**Root Cause Analysis:**

1. **VIX9D** = CBOE's 9-day volatility index (backward-looking, historical)
2. **VIX futures** = Forward market pricing (forward-looking, real money)
3. **VIX options ATM strikes** = Approximate forward expectations (real money proxy)

**VIX9D/VIX ratio ‚â† True term structure**

The ratio of two historical indices doesn't capture forward market expectations. It's a correlation artifact, not a true term structure signal.

**Correct Methodology - VIX Options as Proxy:**

```python
# Get VIX options for multiple expirations
expirations = get_vix_options_expirations()

# For each expiration, find ATM call strike
for exp_date in expirations:
    calls = [c for c in options if c.contract_type == 'call']
    atm_strike = min(calls, key=lambda x: abs(x.strike_price - current_vix))

    # ATM strike ‚âà forward VIX expectation for that date
    forward_expectations.append({
        'expiration': exp_date,
        'forward_vix': atm_strike,
        'dte': days_to_expiration
    })

# Calculate term structure
ts = (forward_vix_1 - current_vix) / current_vix
```

**What We Learned:**

1. ‚úÖ **Always question data sources** - Even when they seem "official"
2. ‚úÖ **Real money > Proxies** - VIX options have skin in the game
3. ‚úÖ **User's domain expertise matters** - Critical thinking caught this
4. ‚úÖ **Validate assumptions** - "VIX term structure" has multiple interpretations
5. ‚úÖ **False signals happen** - VIX9D backwardation was a data artifact

**Action Items - Immediate:**

1. ‚úÖ Stop using VIX9D/VIX as "term structure" in production
2. ‚úÖ Implement VIX options-based term structure calculation
3. ‚úÖ Revise October 2025 risk assessment (90%‚Üí75% confidence)
4. ‚úÖ Update all visualizations to use VIX options data
5. üîÑ Retrain models replacing VIX_Term_Structure feature with VIX_Options_TS

**Action Items - Future:**

1. Create daily VIX options scraper (Polygon API)
2. Build historical VIX options term structure database
3. Backtest using true options-based term structure
4. Document data source decisions with "why we chose this"
5. Regular data quality audits - question everything

**New VIX Term Structure Framework:**

```
VIX Options ATM Strikes Method:
  > +5%:     Strong contango (complacent)
  0% to +5%: Normal contango
  0% to -5%: Slight backwardation (caution)
  -5% to -10%: Backwardation (elevated risk)
  < -10%:    Deep backwardation (HIGH RISK)
```

**Current Reading:** +0.00% (Flat - Neutral expectations)

**Lesson for Future:**
When building financial models, always verify:
- **What does this data actually measure?**
- **Is it forward-looking or backward-looking?**
- **Does it represent real market positioning?**
- **Can we validate with real money markets?**

**Credit:**
This discovery was driven by user's critical questioning. This is exactly the kind of collaborative learning that makes the system better. The user didn't just accept "VIX term structure = -19%" - they asked "wait, is this the right data?"

**Status:** System improved. Risk assessment corrected. Feature engineering updated.

**User Feedback:** ‚≠ê Excellent catch - prevented overconfident false signal

---

### Signal Persistence Framework
**Date Added:** 2025-10-06
**Status:** Partially Validated
**User Feedback:** [Pending]

**Discovery:**
Signal duration matters as much as magnitude. Sustained high probability (>75% for 5+ days) has proven more reliable than single-day spikes.

**Framework:**
```
Single day >70%:     Watch closely (50% confidence)
3+ days >70%:        Take action (70% confidence)
5+ days >75%:        High confidence (85% confidence)
9+ days >75%:        Very high confidence (90%+ confidence)
```

**Historical Validation:**
- July 2024: 5-day persistence ‚Üí -7.4% drop ‚úÖ (Validated)
- October 2025: 9-day persistence ‚Üí [Pending validation]

**Next Steps:**
- Backtest this framework on full historical dataset
- Quantify false positive rate by persistence length
- Develop position sizing algorithm based on persistence
- **Await user feedback** on Oct 2025 outcome to refine confidence levels

**Learning Opportunity:**
If Oct 2025 signal proves false positive despite 9-day persistence:
- Re-examine framework assumptions
- Look for additional filters (e.g., market regime, sentiment)
- Adjust confidence levels
- Document what was different vs July 2024

---

### October 2025 - SPY Options Trade Direction Analysis üéØ
**Date Added:** 2025-10-07
**Status:** ‚è≥ IN PROGRESS - Quote data needed
**Critical Learning:** Volume alone cannot determine dealer positioning

**Key Discovery:** Initial SPY options analysis made speculative claims about dealer positioning without proper trade direction data.

**What We Learned:**

1. **Volume ‚â† Direction**
   - P/C ratio shows volume distribution (valid)
   - But cannot tell if customers buying or selling (invalid without quotes)

2. **Dealer Gamma Sign Matters**
   - Customers buy puts ‚Üí Dealers SHORT puts ‚Üí **Negative gamma**
   - Negative gamma = volatility amplifier (not stabilizer)
   - My initial "dealers long gamma" was backwards

3. **Data Requirements**
   ```
   Trade Direction (BTO/STO): Requires bid/ask quotes
   Dealer Positioning:         Requires bid/ask quotes
   Opening vs Closing:         Requires OI + quotes
   ```

**Scripts Created:**
- `download_and_match_quotes.py` - Downloads 145 GB quote file and matches to trades
- `analyze_spy_flatfiles_complete.py` - 30-day P/C analysis (completed)
- `SPY_OPTIONS_ANALYSIS_SUMMARY_OCT2025.md` - Comprehensive report

**Valid Findings (Without Quotes):**
- ‚úÖ P/C ratio: 1.31 (elevated put volume)
- ‚úÖ Term structure: 1-3M P/C = 3.20 (extreme hedging)
- ‚úÖ Strike distribution: 93.6% ATM calls, 9.3% far OTM puts
- ‚úÖ 30-day anomalies: Sept 11 (P/C=1.68), Sept 18 (P/C=0.59)

**Invalid Claims (Without Quotes):**
- ‚ùå "Dealers long gamma" - Cannot determine
- ‚ùå "Liquidity to downside" - Cannot determine
- ‚ùå Any dealer positioning - Speculation

**Next Steps (Tomorrow):**
1. Download Oct 6 quotes (~145 GB)
2. Match trades to quotes (asof merge by timestamp)
3. Classify BUY vs SELL using bid/ask
4. Calculate actual dealer gamma
5. Update risk assessment

---

### October 2025 - Transparency in Data Limitations üéØ
**Date Added:** 2025-10-07
**Status:** ‚úÖ VALIDATED - Critical Learning
**User Feedback:** Direct correction on improper data claims

**The Problem - Making Claims Without Proper Data:**

When analyzing VIX term structure, I claimed "flat term structure (0.0%)" after correcting the VIX9D/VIX error. However, this was speculation, not data-driven.

**What I Had:**
- VIX options trade volumes and strikes
- No bid/ask quotes
- No VIX futures data

**What I Claimed:**
- "Flat term structure (0.0%)" - This was a guess, not calculated

**The Reality:**
Without VIX futures prices or properly matched VIX options quotes, I **cannot reliably calculate VIX term structure**.

**Why Volume-Weighted Strikes Don't Work:**
```python
# What I tried:
weighted_strike = (calls['strike'] * calls['size']).sum() / calls['size'].sum()
# Result: 34.5 for 16 DTE (implies VIX doubling - clearly wrong)
```

This fails because:
1. Trading volume clusters at certain strikes for many reasons
2. Without quotes, can't identify true ATM strikes
3. Can't distinguish between opening vs closing trades
4. Strike selection bias (some strikes more liquid)

**The Right Approach - Be Transparent:**
‚úÖ **DO:** "I cannot reliably calculate VIX term structure without VIX futures data"
‚úÖ **DO:** "The -19% VIX9D/VIX was incorrect methodology"
‚úÖ **DO:** Explain what data would be needed
‚ùå **DON'T:** Make up numbers to fill gaps
‚ùå **DON'T:** Present speculation as fact

**What's Needed for VIX Term Structure:**
1. **VIX Futures:** Direct forward pricing (most reliable)
2. **VIX Options with Quotes:** ATM strikes approximate forward VIX
3. **VVIX/VIX ratios:** Alternative measure of stress

**Lesson Learned:**
It's better to say "I don't know" or "I can't calculate this reliably" than to make claims without proper data. Users value honesty over false precision.

**Action Items:**
1. ‚úÖ Always state data limitations upfront
2. ‚úÖ Distinguish between what can and cannot be calculated
3. ‚úÖ Explain why certain calculations aren't possible
4. ‚úÖ Suggest what data would be needed
5. ‚úÖ Never interpolate or guess to fill gaps

**User's Key Insight:**
> "If you don't know the answer then it's ok to say you can't calculate it reliably and then explain why"

This is professional integrity - admitting limitations builds trust.

---

### October 2025 - Polygon Quote Download Reality Check üìä
**Date Added:** 2025-10-07
**Status:** ‚úÖ VALIDATED - Performance Reality
**Context:** Attempted to download quotes for dealer positioning

**The Promise vs Reality:**

Initial research suggested Smart Quote Caching would take 16 minutes for ~4,800 API calls.

**What Actually Happened:**
- Script timeout after 20 minutes
- No cache file created
- Process incomplete

**Root Cause Analysis:**

1. **API Rate Limiting Reality:**
   - Theoretical: 5 calls/sec = 4,800 calls in 16 minutes
   - Actual: Network latency + processing = much slower
   - Reality: Probably 1-2 calls/sec effective rate

2. **Scale Miscalculation:**
   - SPY options on Oct 6: 769,207 trades
   - Unique tickers: Likely 4,800+ as estimated
   - But each API call has overhead we didn't account for

**Alternative Approaches Tested:**

1. **VIX Futures via yfinance:** ‚ùå Not available
2. **VIX Variants (VIX9D, VIX3M):** ‚ùå Not true term structure
3. **VIX ETF products:** ‚úÖ Available but not forward-looking

**Key Learning:**
When dealing with financial data APIs at scale:
- Always run a small test first (100 quotes)
- Build in checkpointing/resume capability
- Consider batch processing over multiple runs
- Have fallback data sources ready

**The Reality of Options Data:**
- **Trades:** Easy to get (60MB files)
- **Quotes:** Extremely difficult (145GB files or slow APIs)
- **Implication:** Many analyses simply can't be done without institutional data access

**Action Items:**
1. ‚úÖ Accept that some analyses require institutional data feeds
2. ‚úÖ Focus on what CAN be calculated from trades alone
3. ‚úÖ Document clearly what requires quotes vs what doesn't
4. ‚úÖ Build analyses that degrade gracefully without perfect data

**What We CAN Do Without Quotes:**
- Put/Call ratios ‚úÖ
- Volume analysis ‚úÖ
- Strike/expiry distribution ‚úÖ
- Large trade identification ‚úÖ

**What We CANNOT Do Without Quotes:**
- True IV smile ‚ùå
- Trade direction (BTO/STO) ‚ùå
- Dealer positioning ‚ùå
- VIX forward curve ‚ùå

---
